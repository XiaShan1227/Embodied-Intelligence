### 一、Perception
1. D<sup>3</sup>Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Rearrangement (CoRL-2024) <br>
[[paper]](https://arxiv.org/pdf/2309.16118) [[code]](https://github.com/WangYixuan12/d3fields)

2. UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation (ICRA-2025) <br>
[[paper]](https://arxiv.org/pdf/2506.09284) [[code]](https://github.com/TangYihe/unsup-affordance)

3. Learning Affordance Grounding from Exocentric Images (CVPR-2022) <br>
[[paper]](https://arxiv.org/pdf/2203.09905) [[code]](https://github.com/lhc1224/Cross-View-AG)

4. Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation (CVPR-2025) <br>
[[paper]](https://arxiv.org/pdf/2411.18623) [[code]](https://github.com/PKU-HMI-Lab/LIFT3D?tab=readme-ov-file)

5. AffordanceLLM: Grounding Affordance from Vision Language Models (CVPR-2024) <br>
[[paper]](https://arxiv.org/pdf/2401.06341) [[code]](https://github.com/wj-on-un/AffordanceLLM_implementation?tab=readme-ov-file)

6. DINOv3 <br>
[[paper]](https://arxiv.org/pdf/2508.10104) [[code]](https://github.com/facebookresearch/dinov3?tab=readme-ov-file)

7. SAM 3: Segment Anything with Concepts <br>
[[paper]](https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/) [[code]](https://github.com/facebookresearch/sam3?tab=readme-ov-file)

[[Nan Xue]](https://xuenan.net/) <br>
[[Yuxi Xiao]](https://henry123-boy.github.io/) <br>


#### Depth Completion

1. D<sup>3</sup>RoMa: Disparity Diffusion-based Depth Sensing for Material-Agnostic Robotic Manipulation (CoRL-2024) <br>
[[paper]](https://arxiv.org/pdf/2409.14365) [[code]](https://github.com/songlin/d3roma)

2. Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation (CVPR-2025) <br>
[[paper]](https://promptda.github.io/assets/main_paper_with_supp.pdf) [[code]](https://github.com/DepthAnything/PromptDA)

3. Masked Depth Modeling for Spatial Perception <br>
[[paper]](https://arxiv.org/pdf/2601.17895) [[code]](https://github.com/Robbyant/lingbot-depth)

4. Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots <br>
[[paper]](https://arxiv.org/pdf/2509.02530) [[code]](https://github.com/ByteDance-Seed/manip-as-in-sim-suite)


#### Object Tracking

1. SpatialTracker: Tracking Any 2D Pixels in 3D Space (CVPR-2024) <br>
[[paper]](https://arxiv.org/pdf/2404.04319) [[code]](https://github.com/henry123-boy/SpaTracker?tab=readme-ov-file)

2. SpatialTrackerV2: 3D Point Tracking Made Easy (ICCV-2025) <br>
[[paper]](https://arxiv.org/pdf/2507.12462) [[code]](https://github.com/henry123-boy/SpaTrackerV2)

3. Tracking Any Point <br>
[[Link]](https://github.com/google-deepmind/tapnet)

4. CoTracker: It is Better to Track Together (ECCV-2024) <br>
[[paper]](https://arxiv.org/pdf/2307.07635) [[code]](https://github.com/facebookresearch/co-tracker)

5. PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking (ICCV-2023) <br>
[[paper]](https://arxiv.org/pdf/2307.15055) [[code]](https://github.com/aharley/pips2?tab=readme-ov-file)


### 二、Robotic Manipulation

1. Diffusion Policy: Visuomotor Policy Learning via Action Diffusion (RSS-2023/IJRR-2024) <br>
[[paper]](https://arxiv.org/pdf/2303.04137v4) [[code]](https://github.com/real-stanford/diffusion_policy)

2. 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations (RSS-2024) <br>
[[paper]](https://arxiv.org/pdf/2403.03954) [[code]](https://github.com/YanjieZe/3D-Diffusion-Policy)

3. Generalizable Humanoid Manipulation with 3D Diffusion Policies (IROS-2025) <br>
[[paper]](https://arxiv.org/pdf/2410.10803) [[code]](https://github.com/YanjieZe/Improved-3D-Diffusion-Policy)

4. Motion Before Action: Diffusing Object Motion as Manipulation Condition (RA-L-2025/ICRA-2026) <br>
[[paper]](https://arxiv.org/pdf/2411.09658) [[code]](https://github.com/Selen-Suyue/MBA)

5. GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion Policy (CoRL-2024) <br>
[[paper]](https://robopil.github.io/GenDP/media/main.pdf) [[code]](https://github.com/WangYixuan12/gendp)

6. ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation (CoRL-2024) <br>
[[paper]](https://rekep-robot.github.io/rekep.pdf) [[code]](https://github.com/huangwl18/ReKep)

7. VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models (CoRL-2023) <br>
[[paper]](https://voxposer.github.io/voxposer.pdf) [[code]](https://github.com/huangwl18/VoxPoser)

8. Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes (RA-L-2023) <br>
[[paper]](https://arxiv.org/pdf/2403.18546) [[code]](https://github.com/THU-VCLab/HGGD?tab=readme-ov-file)

9. Diff-DAgger: Uncertainty Estimation with Diffusion Policy for Robotic Manipulation(ICRA-2025) <br>
[[paper]](https://arxiv.org/pdf/2410.14868) [[code]](https://github.com/sean1295/DiffDAgger/tree/main)

[[Wenlong Huang]](https://wenlonghuang.com/) <br>
[[Yanjie Ze]](https://yanjieze.com/) <br>
[[Yixuan Wang]](http://www.yixuanwang.me/) <br>


#### Vision Language Action
[[Link]](https://github.com/MINT-SJTU/Evo-SOTA.io?tab=readme-ov-file)


### 三、Framework

1. [LeRobot](https://github.com/huggingface/lerobot)
2. [Isaac Sim](https://github.com/isaac-sim/IsaacSim)
3. [Isaac Lab](https://github.com/isaac-sim/IsaacLab)
4. [ROS2](https://ros.org/)


### 四、Technical Roadmap
[[Lumina-Embodied-AI-Guide]](https://lumina-embodied.ai/blog/eai-guide)
